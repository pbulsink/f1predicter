% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/metrics.R
\name{pacc_vec}
\alias{pacc_vec}
\alias{pacc}
\alias{pacc.data.frame}
\title{Positive Accuracy metric}
\usage{
pacc_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  case_weights = NULL,
  event_level = "first",
  ...
)

pacc(data, ...)

\method{pacc}{data.frame}(
  data,
  truth,
  estimate,
  ...,
  event_level = yardstick::yardstick_event_level(),
  na_rm = TRUE,
  case_weights = NULL
)
}
\arguments{
\item{truth}{name of the data.frame column containing truth}

\item{estimate}{name of the data.frame column containing the estimates.}

\item{na_rm}{whether to remove NA values}

\item{case_weights}{Not used, required for yardstick metrics}

\item{...}{other elements, passed to yardstick::prob_metric_summarizer()}

\item{data}{data}
}
\value{
a numeric value between 0 and 1 (higher is better)
}
\description{
Subsets the normal accuracy metric to only count the target position(s) (i.e. truth == 1). For example, picking all
0 for win chance would normally give a 95% accuracy (1 in 20 is wrong - 1 in 20 wins). This function only looks at
the accuracy relative to the truth == 1 case (i.e. correctly or not pick the winner). Works for win or for podium
or points models
}
\details{
Idea inspired by https://towardsdatascience.com/formula-1-race-predictor-5d4bfae887da
}
\seealso{
[pacc_vec()] for vector version
}
