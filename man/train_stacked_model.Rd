% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/models_ensemble.R
\name{train_stacked_model}
\alias{train_stacked_model}
\title{Train a Stacked Ensemble Model}
\usage{
train_stacked_model(
  outcome_var,
  model_name,
  train_data,
  data_split,
  data_folds,
  predictor_vars,
  hyperparams,
  model_mode = "regression",
  save_model = TRUE
)
}
\arguments{
\item{outcome_var}{A character string specifying the name of the outcome variable.}

\item{model_name}{A character string for the model's display name (e.g., "Quali Position Ensemble").}

\item{train_data}{The training data frame.}

\item{data_split}{The data split object from `rsample`.}

\item{data_folds}{The cross-validation folds object.}

\item{predictor_vars}{A character vector of predictor variable names.}

\item{hyperparams}{A named list where each name is an engine and the value is
a tibble or list of the optimal hyperparameters for that engine. This is
required.}

\item{model_mode}{A character string, either `"regression"` or `"classification"`.}

\item{save_model}{A logical value. If `TRUE` (default), the trained ensemble model
is automatically butchered and saved to the path specified in `options('f1predicter.models')`.}
}
\value{
A fitted `model_stack` object, ready for prediction.
}
\description{
This function trains multiple model types (e.g., ranger, glmnet, kknn) for a
single prediction task and combines them into a stacked ensemble model using
the `stacks` package. It uses pre-defined hyperparameters for each model,
bypassing the tuning step.
}
\details{
The function iterates through a provided list of `engines`. For each engine,
it defines a `parsnip` model specification, finalizes it with the provided
hyperparameters, and then fits the model to resamples using
`tune::fit_resamples()` with a special control setting
(`control_stack_resamples()`) that saves the out-of-fold predictions
necessary for stacking.

After all candidate models are tuned, they are added to a `data_stack`.
The `blend_predictions()` function is called to find the optimal linear
combination of the member models. Finally, `fit_members()` fits the final
ensemble model.

This approach often yields a model with better predictive performance than any
of the individual member models.
}
\examples{
\dontrun{
# This is a conceptual example.
# You would first need to prepare your data, splits, and folds.

# optimal_params <- list(
#   ranger = tibble::tibble(mtry = 5, min_n = 10),
#   glmnet = tibble::tibble(penalty = 0.01, mixture = 0.5)
# )

# data <- clean_data()
# ... (create splits, folds, predictor_vars)

# quali_pos_ensemble <- train_stacked_model(
#   outcome_var = "quali_position",
#   model_name = "Quali Position Ensemble",
#   train_data = my_train_data,
#   data_split = my_data_split,
#   data_folds = my_data_folds,
#   predictor_vars = my_predictor_vars,
#   hyperparams = optimal_params,
#   model_mode = "regression"
# )

# # Make predictions
# predictions <- stats::predict(quali_pos_ensemble, new_data = my_test_data)
}
}
